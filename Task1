import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Load the Titanic dataset
file_path = "â€ªC:\Users\DELL\Downloads\tested.csv"
data = pd.read_excel(file_path)

# Data Exploration and Visualization
print(data.info())
print(data.describe())

# Data Preprocessing
data.drop(['Name', 'Ticket', 'Cabin', 'Embarked'], axis=1, inplace=True)
data['Sex'] = data['Sex'].map({'male': 0, 'female': 1})
data['Age'].fillna(data['Age'].median(), inplace=True)
data['Fare'].fillna(data['Fare'].median(), inplace=True)

# Visualize the survival distribution by features
sns.set(style="whitegrid", font_scale=1.2)
plt.figure(figsize=(12, 10))
plt.subplot(3, 2, 1)
sns.countplot(x='Survived', data=data)
plt.subplot(3, 2, 2)
sns.countplot(x='Pclass', hue='Survived', data=data)
plt.subplot(3, 2, 3)
sns.countplot(x='Sex', hue='Survived', data=data)
plt.subplot(3, 2, 4)
sns.countplot(x='SibSp', hue='Survived', data=data)
plt.subplot(3, 2, 5)
sns.countplot(x='Parch', hue='Survived', data=data)
plt.subplot(3, 2, 6)
sns.histplot(data['Age'][data['Survived'] == 1], color='skyblue', label='Survived', kde=True)
sns.histplot(data['Age'][data['Survived'] == 0], color='salmon', label='Not Survived', kde=True)
plt.legend()
plt.tight_layout()
plt.show()

# Prepare data for modeling
X = data.drop('Survived', axis=1)
y = data['Survived']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Build and train the logistic regression model
logistic_model = LogisticRegression()
logistic_model.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_pred_logistic = logistic_model.predict(X_test_scaled)

# Model evaluation for logistic regression
accuracy_logistic = accuracy_score(y_test, y_pred_logistic)
conf_matrix_logistic = confusion_matrix(y_test, y_pred_logistic)
class_report_logistic = classification_report(y_test, y_pred_logistic)

print("Logistic Regression:")
print("Accuracy:", accuracy_logistic)
print("Confusion Matrix:\n", conf_matrix_logistic)
print("Classification Report:\n", class_report_logistic)

# Build and train the random forest model
random_forest_model = RandomForestClassifier(random_state=42)
random_forest_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred_rf = random_forest_model.predict(X_test)

# Model evaluation for random forest
accuracy_rf = accuracy_score(y_test, y_pred_rf)
conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)
class_report_rf = classification_report(y_test, y_pred_rf)

print("Random Forest:")
print("Accuracy:", accuracy_rf)
print("Confusion Matrix:\n", conf_matrix_rf)
print("Classification Report:\n", class_report_rf)

# Hyperparameter Tuning for Random Forest
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 5, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(estimator=random_forest_model, param_grid=param_grid, cv=5)
grid_search.fit(X_train, y_train)

best_params = grid_search.best_params_
best_random_forest_model = grid_search.best_estimator_

print("Best Hyperparameters for Random Forest:")
print(best_params)

# Cross-validation for the best Random Forest model
cv_accuracy = cross_val_score(best_random_forest_model, X_train, y_train, cv=5, scoring='accuracy')
mean_cv_accuracy = np.mean(cv_accuracy)

print("Cross-Validation Accuracy for the best Random Forest model:", mean_cv_accuracy)
